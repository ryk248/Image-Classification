{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name - Rahul Keshwani\n",
    "NetID - ryk248\n",
    "\n",
    "Name - Arun Kodnani\n",
    "NetId - ak6384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "HkIC2u2oZs_K"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "pMjhmzzTZzoj",
    "outputId": "299f5a87-0508-4325-ad0f-0d1e225a92ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "#Getting the data and applying transformation\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "indices = list(range(len(trainset)))\n",
    "split = (int)(0.1*len(trainset))\n",
    "\n",
    "#Splitting indices for train and validation\n",
    "validation_indices = np.random.choice(indices, size=split, replace=False)\n",
    "train_indices = list(set(indices) - set(validation_indices))\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "validation_sampler = SubsetRandomSampler(validation_indices)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, num_workers=2, sampler=train_sampler)\n",
    "validationloader = torch.utils.data.DataLoader(trainset, batch_size=1, num_workers=2, sampler=validation_sampler)\n",
    "testloader = torch.utils.data.DataLoader(testset, shuffle=True, batch_size=1, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are here creating a dense CNN instead of a small network with very large hidden layers because that would save the \n",
    "number of parameters of the network.\n",
    "\n",
    "Following is a paper on ImageNet that we have used as a reference for our architecture. \n",
    "http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "lfOlozjCZ3Oo"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        #Convolutional layer 1. Input channels=3, Output channels=16.\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        #Convolutional layer 2. Input channels=16, Output channels=32. \n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        #Convolutional layer 3. Input channels=32, Output channels=64\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        #Convolutional layer 4. Input channels=64, Output channels=128\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        #Convolutional layer 5. Input channels=128, Output channels=256\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Pooling layer \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        #Fully connected layer 1\n",
    "        self.fc1 = nn.Linear(256 * 4 * 4, 1024)\n",
    "        \n",
    "        #Fully connected layer 2\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        \n",
    "        #Fully connected layer 3\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Convolution 1 and activation. Size changes from (3,32,32) to (16,32,32)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        \n",
    "        #Downsampling. Size changes from (16,32,32) to (16,16,16)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #Convolution 2 and activation. Size changes from (16,16,16) to (32,16,16)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        \n",
    "        #Downsampling. Size changes from (32,16,16) to (32,8,8)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #Convolution 3 and activation. Size changes from (32,8,8) to (64,8,8)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        \n",
    "        #Convolution 4 and activation. Size changes from (64,8,8) to (128,8,8)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        \n",
    "        #Convolution 5 and activation. Size changes from (128,8,8) to (256,8,8)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        \n",
    "        #Downsampling. Size changes from (256,8,8) to (256,4,4)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #Flatten data for fully connected layer. size changes from (256,4,4) to (1,4096)\n",
    "        x = x.view(-1,256 * 4 * 4)\n",
    "        \n",
    "        #First fully connected layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        #Second fully connected layer\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        #Third fully connected layer. \n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For calculating the loss we have used \"CrossEntropyLoss\" which automatically applies Softmax before calculating the loss and hence we have not added a softmax layer in our architecture. This was one of the main reason we have selected this particular loss function.\n",
    "\n",
    "We have used \"Adam Optimizer\" because it works really well with Non-Convex functions where it is very easy to land on the local optima. One of the main reason of using this optimizer is that it maintains a learning rate for each of the parameters and keeps it adaptive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "UWm37nc3aVHm"
   },
   "outputs": [],
   "source": [
    "def trainCNNModel(num_epochs, learning_rate):\n",
    "    #Creating an object of CNN class to build the network\n",
    "    cnn_model = CNN()\n",
    "    \n",
    "    cnn_model.cuda()\n",
    "    \n",
    "    #Defining the loss function and optimizer\n",
    "    loss_method = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(cnn_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    #Check if GPU is available\n",
    "    if torch.cuda.is_available():\n",
    "        cnn_model = cnn_model.cuda()\n",
    "        loss_method = loss_method.cuda()\n",
    "\n",
    "    #Iterate over the input images multiple times \n",
    "    for epoch in range(num_epochs):\n",
    "        training_loss = 0.0\n",
    "        for i, (train_images, train_labels) in enumerate(trainloader):\n",
    "#             train_images, train_labels = Variable(train_images), Variable(train_labels)\n",
    "            \n",
    "            train_images, train_labels = train_images.cuda(), train_labels.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_output = cnn_model.forward(train_images)\n",
    "            loss = loss_method(train_output, train_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            training_loss += loss.item()\n",
    "            if i % 50 == 49:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, training_loss / 50))\n",
    "                training_loss = 0.0\n",
    "            \n",
    "    return cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "3YiPyIx0oXa1",
    "outputId": "0d1f60a2-6f53-4ef8-ce91-c47a22f3ae16"
   },
   "outputs": [],
   "source": [
    "def testCNNModel(cnn_model):\n",
    "    total = 0\n",
    "    correct_predictions = 0\n",
    "    predictions = []\n",
    "    #Sets the \"requires_grad\" flag to False to avoid gradient descent during predictions\n",
    "    with torch.no_grad():\n",
    "        #Iterate over the test images\n",
    "        for i, (test_images, test_labels) in enumerate(testloader):\n",
    "            test_images, test_labels = Variable(test_images), Variable(test_labels)\n",
    "            test_output = cnn_model(test_images)\n",
    "#             max_value, prediction_class = torch.max(test_output.data, 1)\n",
    "            \n",
    "            pred_y = torch.max(test_output, 1)[1].cuda().data.squeeze() \n",
    "            accuracy = torch.sum(pred_y == test_y).type(torch.FloatTensor) / test_y.size(0)\n",
    "            \n",
    "            predictions.extend(prediction_class)\n",
    "            total += test_labels.size(0)\n",
    "            correct_predictions += torch.sum(prediction_class == test_labels)\n",
    "            \n",
    "    return total, np.array(correct_predictions), predictions\n",
    "\n",
    "def calculateAccuracy(total, correct_predictions):\n",
    "    return 100 * (correct_predictions/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1oit80ueIV9R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 2.170\n",
      "[1,   100] loss: 1.937\n",
      "[1,   150] loss: 1.825\n",
      "[1,   200] loss: 1.696\n",
      "[1,   250] loss: 1.589\n",
      "[1,   300] loss: 1.502\n",
      "[1,   350] loss: 1.422\n",
      "[1,   400] loss: 1.389\n",
      "[1,   450] loss: 1.340\n",
      "[2,    50] loss: 1.293\n",
      "[2,   100] loss: 1.271\n",
      "[2,   150] loss: 1.234\n",
      "[2,   200] loss: 1.205\n",
      "[2,   250] loss: 1.161\n",
      "[2,   300] loss: 1.160\n",
      "[2,   350] loss: 1.136\n",
      "[2,   400] loss: 1.115\n",
      "[2,   450] loss: 1.068\n",
      "[3,    50] loss: 1.025\n",
      "[3,   100] loss: 0.995\n",
      "[3,   150] loss: 0.997\n",
      "[3,   200] loss: 0.999\n",
      "[3,   250] loss: 0.959\n",
      "[3,   300] loss: 0.939\n",
      "[3,   350] loss: 0.928\n",
      "[3,   400] loss: 0.948\n",
      "[3,   450] loss: 0.941\n",
      "[4,    50] loss: 0.849\n",
      "[4,   100] loss: 0.853\n",
      "[4,   150] loss: 0.832\n",
      "[4,   200] loss: 0.820\n",
      "[4,   250] loss: 0.785\n",
      "[4,   300] loss: 0.822\n",
      "[4,   350] loss: 0.819\n",
      "[4,   400] loss: 0.796\n",
      "[4,   450] loss: 0.827\n",
      "[5,    50] loss: 0.719\n",
      "[5,   100] loss: 0.685\n",
      "[5,   150] loss: 0.720\n",
      "[5,   200] loss: 0.704\n",
      "[5,   250] loss: 0.716\n",
      "[5,   300] loss: 0.702\n",
      "[5,   350] loss: 0.698\n",
      "[5,   400] loss: 0.688\n",
      "[5,   450] loss: 0.669\n",
      "Accuracy of the network for learning rate =  0.001 is 71.65 %\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.001]\n",
    "for lr in learning_rates:\n",
    "    cnn_model = trainCNNModel(5, lr)\n",
    "    total, correct_predictions, predictions = testCNNModel(cnn_model)\n",
    "    accuracy = calculateAccuracy(total, correct_predictions)\n",
    "    print(\"Accuracy of the network for learning rate = \", lr, \"is\", accuracy, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_predictions(filename, y):\n",
    "    np.save(filename, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_predictions('ans2-ryk248', predictions)\n",
    "save_predictions('ans2-ak6384', predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
